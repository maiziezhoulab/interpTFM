{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53625092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huy21/anaconda3/envs/scgpt/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from compare_activations_to_concepts import (\n",
    "    load_concept_names,\n",
    ")\n",
    "\n",
    "\n",
    "def load_metadata(eval_set_dir: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load and return evaluation set metadata from JSON file.\"\"\"\n",
    "    with open(eval_set_dir / \"metadata.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def calculate_metrics(\n",
    "    tp: np.ndarray,\n",
    "    fp: np.ndarray,\n",
    "    fn: np.ndarray,\n",
    "    tp_per_domain: np.ndarray,\n",
    "    positive_labels: np.ndarray,\n",
    "    # positive_labels_per_domain: np.ndarray,\n",
    "    concept_names: List[str],\n",
    "    threshold_percents: List[float],\n",
    "    # is_aa_concept_list: List[bool],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1 scores for each concept-feature-threshold combination.\n",
    "\n",
    "    Args:\n",
    "        tp: True positives array (concepts x features x thresholds)\n",
    "        fp: False positives array\n",
    "        tp_per_domain: True positives per domain array\n",
    "        positive_labels: Total positive labels per concept\n",
    "        positive_labels_per_domain: Total positive labels per domain per concept\n",
    "        concept_names: List of concept names\n",
    "        threshold_percents: List of threshold percentages\n",
    "        # is_aa_concept_list: Boolean list indicating if each concept is AA-level\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing calculated metrics for each combination\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # for concept_idx, concept in enumerate(concept_names):\n",
    "    #     actual_positives = positive_labels[concept_idx]\n",
    "    #     for feature in range(tp.shape[1]):\n",
    "    #         for threshold_idx in range(len(threshold_percents)):\n",
    "    #             this_tp = tp[concept_idx, feature, threshold_idx]\n",
    "    #             if actual_positives > 0 and this_tp > actual_positives:\n",
    "    #                 print(f\"[ERROR] Concept: {concept}, TP: {this_tp}, Positives: {actual_positives}, Recall: {this_tp / actual_positives}\")\n",
    "\n",
    "\n",
    "    for concept_idx, concept in enumerate(concept_names):\n",
    "        for feature in range(tp.shape[1]):\n",
    "            for threshold_idx, threshold_pct in enumerate(threshold_percents):\n",
    "                # Skip if no true positives\n",
    "                if tp[concept_idx, feature, threshold_idx] == 0:\n",
    "                    continue\n",
    "\n",
    "                # Calculate tp, fp, precision and recall\n",
    "                curr_tp = tp[concept_idx, feature, threshold_idx]\n",
    "                curr_fp = fp[concept_idx, feature, threshold_idx]\n",
    "                curr_fn = fn[concept_idx, feature, threshold_idx]\n",
    "                precision = curr_tp / (curr_tp + curr_fp)\n",
    "                recall = curr_tp / (curr_tp + curr_fn)\n",
    "                # recall = (\n",
    "                #     curr_tp / positive_labels[concept_idx]\n",
    "                #     if positive_labels[concept_idx] > 0\n",
    "                #     else 0\n",
    "                # )  # we dont have FNs!!!\n",
    "\n",
    "                # Calculate recall per domain for domain-level concepts or just\n",
    "                # use recall if AA-level concept\n",
    "                # if is_aa_concept_list[concept_idx]:\n",
    "                #     recall_per_domain = recall\n",
    "                # else:\n",
    "                # recall_per_domain = (\n",
    "                #     tp_per_domain[concept_idx, feature, threshold_idx]\n",
    "                #     / positive_labels_per_domain[concept_idx]\n",
    "                #     if positive_labels_per_domain[concept_idx] > 0\n",
    "                #     else 0\n",
    "                # )\n",
    "\n",
    "                # Calculate F1 scores\n",
    "                f1 = calculate_f1(precision, recall)\n",
    "                # f1_per_domain = calculate_f1(precision, recall_per_domain)\n",
    "\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"concept\": concept,\n",
    "                        \"feature\": feature,\n",
    "                        \"threshold_pct\": threshold_pct,\n",
    "                        \"precision\": precision,\n",
    "                        \"recall\": recall,\n",
    "                        \"f1\": f1,\n",
    "                        \"tp\": curr_tp,\n",
    "                        \"fp\": curr_fp,\n",
    "                        \"tp_per_domain\": tp_per_domain[\n",
    "                            concept_idx, feature, threshold_idx\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def calculate_f1(precision: float, recall: float) -> float:\n",
    "    \"\"\"Calculate F1 score from precision and recall.\"\"\"\n",
    "    return (\n",
    "        2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1cd7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_metrics_across_shards(\n",
    "    eval_res_dir: Path,\n",
    "    eval_set_dir: Path,\n",
    "    threshold_percents: List[float] = [0, 0.15, 0.5, 0.6, 0.8],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Combine metrics across multiple evaluation shards and save results.\n",
    "\n",
    "    Args:\n",
    "        eval_res_dir: Directory containing evaluation results\n",
    "        eval_set_dir: Directory containing evaluation set data\n",
    "        threshold_percents: List of threshold percentages to evaluate\n",
    "        shards_to_eval: Optional list of specific shards to evaluate\n",
    "    \"\"\"\n",
    "    # Load concept information\n",
    "    concept_names = load_concept_names(eval_set_dir / \"gene_concepts_columns.txt\")\n",
    "    # is_aa_concept_list = [is_aa_level_concept(name) for name in concept_names]\n",
    "\n",
    "    # Load metadata and get positive label counts\n",
    "    metadata = load_metadata(eval_set_dir)\n",
    "    positive_labels = np.array(metadata[\"n_positive_gene_per_concept\"])\n",
    "    # positive_labels_per_domain = np.array(metadata[\"n_positive_domains_per_concept\"])\n",
    "\n",
    "    # Use all shards if none specified\n",
    "    shards_to_eval = metadata[\"shard_source\"]\n",
    "\n",
    "    # Initialize total counts\n",
    "    tp_total = None\n",
    "    fp_total = None\n",
    "    fn_total = None\n",
    "    tp_per_domain_total = None\n",
    "\n",
    "    # Combine counts from all shards\n",
    "    for shard in tqdm(shards_to_eval, desc=\"Combining shard counts\"):\n",
    "        shard_data = np.load(eval_res_dir / f\"shard_{shard}_counts.npz\")\n",
    "\n",
    "        # For the first shard, initialize total counts with correct shapes\n",
    "        if tp_total is None:\n",
    "            tp_total = np.zeros(shard_data[\"tp\"].shape)\n",
    "            fp_total = np.zeros(shard_data[\"fp\"].shape)\n",
    "            fn_total = np.zeros(shard_data[\"fn\"].shape)\n",
    "            tp_per_domain_total = np.zeros(shard_data[\"tp_per_domain\"].shape)\n",
    "\n",
    "        tp_total += shard_data[\"tp\"]\n",
    "        fp_total += shard_data[\"fp\"]\n",
    "        fn_total += shard_data[\"fn\"]\n",
    "        tp_per_domain_total += shard_data[\"tp_per_domain\"]\n",
    "\n",
    "    # Calculate and save metrics\n",
    "    print(\"Calculating F1 scores...\")\n",
    "    metrics_df = calculate_metrics(\n",
    "        tp_total,\n",
    "        fp_total,\n",
    "        fn_total,\n",
    "        tp_per_domain_total,\n",
    "        np.array(positive_labels).sum(axis=0),\n",
    "        # np.array(positive_labels_per_domain).sum(axis=0),\n",
    "        concept_names,\n",
    "        threshold_percents\n",
    "    )\n",
    "\n",
    "    output_path = eval_set_dir / \"concept_f1_scores.csv\"\n",
    "    metrics_df.to_csv(output_path, index=False)\n",
    "    print(f\"Metrics saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a56cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining shard counts: 100%|██████████| 3/3 [00:12<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F1 scores...\n",
      "Metrics saved to /maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output/valid/concept_f1_scores.csv\n"
     ]
    }
   ],
   "source": [
    "eval_res_dir = Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output')\n",
    "eval_set_dir = Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output/valid')\n",
    "combine_metrics_across_shards(eval_res_dir, eval_set_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9a3c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining shard counts: 100%|██████████| 2/2 [00:09<00:00,  4.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F1 scores...\n",
      "Metrics saved to /maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output/test/concept_f1_scores.csv\n"
     ]
    }
   ],
   "source": [
    "eval_res_dir = Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output')\n",
    "eval_set_dir = Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output/test')\n",
    "combine_metrics_across_shards(eval_res_dir, eval_set_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c61ff663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/maiziezhou_lab2/yunfei/Projects/FM_temp/interGFM/output/test/concept_f1_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77f230f",
   "metadata": {},
   "source": [
    "# for acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ff76d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining shard counts: 100%|██████████| 3/3 [00:02<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F1 scores...\n",
      "Metrics saved to /maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output_acts/valid/concept_f1_scores.csv\n"
     ]
    }
   ],
   "source": [
    "eval_res_dir = Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output_acts')\n",
    "eval_set_dir = Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output_acts/valid')\n",
    "combine_metrics_across_shards(eval_res_dir, eval_set_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aabb6c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining shard counts: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F1 scores...\n",
      "Metrics saved to /maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output_acts/test/concept_f1_scores.csv\n"
     ]
    }
   ],
   "source": [
    "eval_res_dir = Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output_acts')\n",
    "eval_set_dir = Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output_acts/test')\n",
    "combine_metrics_across_shards(eval_res_dir, eval_set_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
