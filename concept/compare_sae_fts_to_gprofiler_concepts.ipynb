{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5d472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Optional\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('/maiziezhou_lab2/yunfei/Projects/interpTFM/sae')\n",
    "from dictionary import AutoEncoder\n",
    "from inference import get_sae_feats_in_batches, load_sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f63ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_nonzero_dense(matrix: torch.Tensor) -> List[int]:\n",
    "    \"\"\"\n",
    "    Count unique non-zero values in each column of a dense matrix.\n",
    "\n",
    "    Args:\n",
    "        matrix: Dense PyTorch tensor to analyze\n",
    "\n",
    "    Returns:\n",
    "        List of counts of unique non-zero values for each column\n",
    "    \"\"\"\n",
    "    # Initialize list to store counts\n",
    "    unique_counts = []\n",
    "\n",
    "    # Iterate through each column\n",
    "    for col in range(matrix.shape[1]):\n",
    "        # Get unique values in the column\n",
    "        unique_values = torch.unique(matrix[:, col])\n",
    "        # Count how many unique values are non-zero\n",
    "        count = torch.sum(unique_values != 0).item()\n",
    "        unique_counts.append(count)\n",
    "\n",
    "    return unique_counts\n",
    "\n",
    "\n",
    "def calc_metrics_dense(\n",
    "    sae_feats: torch.Tensor,\n",
    "    per_token_labels_sparse: Union[np.ndarray, sparse.spmatrix],\n",
    "    threshold_percents: List[float],\n",
    "    is_aa_level_concept: List[bool],\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Optimized GPU-compatible metric computation for dense matrices.\n",
    "    \"\"\"\n",
    "    device = sae_feats.device\n",
    "    labels = torch.tensor(per_token_labels_sparse.astype(np.float32), device=device)  # [N, C]\n",
    "    N, F = sae_feats.shape\n",
    "    C = labels.shape[1]\n",
    "    T = len(threshold_percents)\n",
    "\n",
    "    # Thresholds as tensor [T, 1, 1] for broadcasting\n",
    "    thresholds = torch.tensor(threshold_percents, dtype=torch.float32, device=device).view(T, 1, 1)\n",
    "\n",
    "    # Expand and binarize features: [T, N, F]\n",
    "    feats_exp = sae_feats.unsqueeze(0)  # [1, N, F]\n",
    "    bin_feats = (feats_exp > thresholds).float()  # [T, N, F]\n",
    "\n",
    "    # Labels: [1, N, C]\n",
    "    labels_exp = labels.T.unsqueeze(0)  # [1, C, N]\n",
    "\n",
    "    # Calculate TP: [T, C, F]\n",
    "    tp = torch.matmul(labels_exp, bin_feats)  # [T, C, F]\n",
    "    tp = tp.permute(1, 2, 0).contiguous()  # [C, F, T]\n",
    "\n",
    "    # Calculate FP: [T, C, F]\n",
    "    not_labels_exp = (1.0 - labels.T).unsqueeze(0)  # [1, C, N]\n",
    "    fp = torch.matmul(not_labels_exp, bin_feats)  # [T, C, F]\n",
    "    fp = fp.permute(1, 2, 0).contiguous()  # [C, F, T]\n",
    "\n",
    "    # Calculate TP per domain for non-AA-level only\n",
    "    tp_per_domain = torch.zeros_like(tp)\n",
    "\n",
    "    # non_aa_indices = [i for i, flag in enumerate(is_aa_level_concept) if not flag]\n",
    "    # if non_aa_indices:\n",
    "    #     non_aa_mask = torch.zeros(C, dtype=torch.bool, device=device)\n",
    "    #     non_aa_mask[non_aa_indices] = True\n",
    "\n",
    "    #     # For non-AA concepts: compute domain-level TP (number of examples with ≥1 positive feature)\n",
    "    #     for t_idx in range(T):\n",
    "    #         # For each threshold: binary_feats [N, F], labels [N, C]\n",
    "    #         bf = bin_feats[t_idx]  # [N, F]\n",
    "    #         l = labels  # [N, C]\n",
    "\n",
    "    #         # Multiply elementwise [N, F] * [N, C] -> [N, C, F]\n",
    "    #         combined = (bf.unsqueeze(1) * l.unsqueeze(2))  # [N, C, F]\n",
    "    #         per_domain_tp = (combined.sum(dim=0) > 0).float()  # [C, F]\n",
    "    #         tp_per_domain[:, :, t_idx] = per_domain_tp\n",
    "\n",
    "    positive_labels = labels.sum(dim=0)  # [C]\n",
    "    positive_labels = positive_labels.view(-1, 1, 1)  # [C, 1, 1]\n",
    "\n",
    "    fn = positive_labels - tp  # [C, F, T]\n",
    "    fn = torch.clamp(fn, min=0)  # Optional, to avoid negative values due to float precision\n",
    "\n",
    "    return tp.cpu().numpy(), fp.cpu().numpy(), fn.cpu().numpy(), tp_per_domain.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "834035c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_concept_names(concept_name_path: Path) -> List[str]:\n",
    "    \"\"\"Load concept names from a file.\"\"\"\n",
    "    with open(concept_name_path, \"r\") as f:\n",
    "        return f.read().strip().split(\"\\n\")\n",
    "\n",
    "\n",
    "def process_shard(\n",
    "    sae: AutoEncoder,\n",
    "    device: torch.device,\n",
    "    esm_embeddings_pt_path: str,\n",
    "    per_token_labels: Union[np.ndarray, sparse.spmatrix],\n",
    "    threshold_percents: List[float],\n",
    "    is_aa_concept_list: List[bool],\n",
    "    feat_chunk_max: int = 512,\n",
    "    is_sparse: bool = False,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Process a shard of data by splitting it into manageable chunks for feature calculation.\n",
    "\n",
    "    Args:\n",
    "        sae: Normalized SAE model\n",
    "        device: PyTorch device to use for computation\n",
    "        esm_embeddings_pt_path: Path to ESM embeddings file\n",
    "        per_token_labels: Label matrix\n",
    "        threshold_percents: List of threshold values to evaluate\n",
    "        is_aa_concept_list: Boolean flags indicating if each concept is AA-level\n",
    "        feat_chunk_max: Maximum chunk size for feature processing\n",
    "        is_sparse: Whether to use sparse matrix operations\n",
    "\n",
    "    Returns:\n",
    "        Tuple of arrays (tp, fp, tp_per_domain) containing calculated metrics\n",
    "    \"\"\"\n",
    "    # Load embeddings to specified device\n",
    "    esm_acts = torch.load(\n",
    "        esm_embeddings_pt_path, map_location=device, weights_only=True\n",
    "    )\n",
    "\n",
    "    # Calculate chunking parameters\n",
    "    feature_chunk_size = min(feat_chunk_max, sae.dict_size)\n",
    "    total_features = sae.dict_size\n",
    "    num_chunks = int(np.ceil(total_features / feature_chunk_size))\n",
    "    print(f\"Calculating over {total_features} features in {num_chunks} chunks\")\n",
    "\n",
    "    # Initialize result arrays\n",
    "    n_concepts = per_token_labels.shape[1]\n",
    "    n_thresholds = len(threshold_percents)\n",
    "    n_features = sae.dict_size\n",
    "    tp = np.zeros((n_concepts, n_features, n_thresholds))\n",
    "    fp = np.zeros((n_concepts, n_features, n_thresholds))\n",
    "    fn = np.zeros((n_concepts, n_features, n_thresholds))\n",
    "    tp_per_domain = np.zeros((n_concepts, n_features, n_thresholds))\n",
    "\n",
    "    # Convert labels to appropriate format\n",
    "    # per_token_labels = (\n",
    "    #     sparse.csr_matrix(per_token_labels) if is_sparse else per_token_labels.toarray()\n",
    "    # )\n",
    "\n",
    "    # Process each chunk of features\n",
    "    for feature_list in tqdm(np.array_split(range(total_features), num_chunks)):\n",
    "        # Get SAE features for current chunk\n",
    "        sae_feats = get_sae_feats_in_batches(\n",
    "            sae=sae,\n",
    "            device=device,\n",
    "            esm_embds=esm_acts,\n",
    "            chunk_size=1024,\n",
    "            feat_list=feature_list,\n",
    "        )\n",
    "\n",
    "        # Calculate metrics using either sparse or dense implementation\n",
    "        # if is_sparse:\n",
    "        #     sae_feats_sparse = sparse.csr_matrix(sae_feats.cpu().numpy())\n",
    "        #     metrics = calc_metrics_sparse(\n",
    "        #         sae_feats_sparse,\n",
    "        #         per_token_labels,\n",
    "        #         threshold_percents,\n",
    "        #         is_aa_concept_list,\n",
    "        #     )\n",
    "        # else:\n",
    "        metrics = calc_metrics_dense(\n",
    "            sae_feats, per_token_labels, threshold_percents, is_aa_concept_list\n",
    "        )\n",
    "\n",
    "        # Update results arrays with computed metrics\n",
    "        tp_subset, fp_subset, fn_subset, tp_per_domain_subset = metrics\n",
    "        tp[:, feature_list] = tp_subset\n",
    "        fp[:, feature_list] = fp_subset\n",
    "        fn[:, feature_list] = fn_subset\n",
    "        tp_per_domain[:, feature_list] = tp_per_domain_subset\n",
    "\n",
    "    return (tp, fp, fn, tp_per_domain)\n",
    "\n",
    "\n",
    "# def analyze_concepts(\n",
    "#     adata_path: Path,\n",
    "#     gene_ids_path: Path,\n",
    "#     concepts_path: Path,\n",
    "#     gene_ignore: List,\n",
    "#     sae_dir: Path,\n",
    "#     esm_embds_dir: Path = Path(\"../../data/processed/embeddings\"),\n",
    "#     eval_set_dir: Path = Path(\"../../data/processed/valid\"),\n",
    "#     output_dir: Path = \"concept_results\",\n",
    "#     threshold_percents: List[float] = [0, 0.15, 0.5, 0.6, 0.8],\n",
    "#     shard: Optional[str] = None, # 'shard_55'\n",
    "#     is_sparse: bool = True,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Analyzes concepts in protein sequences using a Sparse Autoencoder (SAE) model.\n",
    "\n",
    "#     Args:\n",
    "#         sae_dir (Path): Directory containing the normalized SAE model file 'ae_normalized.pt'\n",
    "#         esm_embds_dir (Path, optional): Directory containing ESM embeddings.\n",
    "#         eval_set_dir (Path, optional): Directory containing validation dataset and metadata.\n",
    "#         output_dir (Path, optional): Directory where results will be saved.\n",
    "#         threshold_percents (List[float], optional): List of threshold values for concept detection.\n",
    "#         shard (int | None): Specific shard number to process. Must exist in evaluation set.\n",
    "#         is_sparse (bool, optional): Whether to use sparse matrix operations.\n",
    "\n",
    "#     Returns:\n",
    "#         None: Results are saved to disk as NPZ file with following arrays:\n",
    "#             - tp: True positives counts\n",
    "#             - fp: False positives counts\n",
    "#             - tp_per_domain: True positives counts per domain\n",
    "\n",
    "#     Raises:\n",
    "#         ValueError: If normalized SAE model is not found in sae_dir\n",
    "#         ValueError: If specified shard is not in the evaluation set\n",
    "#     \"\"\"\n",
    "\n",
    "\n",
    "#     # Verify that the normalized SAE model exists\n",
    "#     if not (sae_dir / \"ae_normalized.pt\").exists():\n",
    "#         raise ValueError(f\"Normalized SAE model not found in {sae_dir}\")\n",
    "    \n",
    "#     ad_ = sc.read_h5ad(adata_path)\n",
    "\n",
    "#     # keep for only current shard\n",
    "#     ad_subset = ad_[ad_.obs[\"shards\"] == shard].copy()\n",
    "\n",
    "#     # remove genes not in vocab\n",
    "#     if \"index\" in ad_subset.var.columns:\n",
    "#         genes_to_keep = ~ad_subset.var[\"index\"].isin(gene_ignore)\n",
    "#     ad_subset = ad_subset[:, genes_to_keep]\n",
    "\n",
    "#     # print(shard)\n",
    "#     # file_path = \"/maiziezhou_lab2/yunfei/Projects/FM_temp/InterPLM/interplm/scgpt/gene_ids/shard_0/all_input_gene_ids.txt\"\n",
    "#     with open(gene_ids_path / shard / \"cell_gene_pairs.txt\", \"r\") as f:\n",
    "#         gene_ids = f.read().split()\n",
    "    \n",
    "#     # Load the binary concept-gene matrix\n",
    "#     df = pd.read_csv(concepts_path / 'gene_concepts.csv', index_col=0)  # Set index_col=0 if the first column is a concept name\n",
    "\n",
    "#     # Create a dictionary: gene (column) ➜ list of 0/1 for all concepts\n",
    "#     gene_to_concepts = {gene: df[gene].tolist() for gene in df.columns}\n",
    "#     concept_names = load_concept_names(concepts_path / \"gprofiler_gene_concepts_columns.txt\")\n",
    "#     # print(concept_names)\n",
    "#     per_token_labels = np.zeros((len(gene_ids), len(concept_names)))\n",
    "\n",
    "#     # print(per_token_labels.shape)\n",
    "\n",
    "    \n",
    "\n",
    "#     for i, gene in enumerate(gene_ids):\n",
    "#         if gene in gene_to_concepts:\n",
    "#             # print(len(gene_to_concepts[gene]))\n",
    "#             # print(gene_to_concepts[gene])\n",
    "#             per_token_labels[i] = gene_to_concepts[gene]\n",
    "#         else:\n",
    "#             # Keep all-zero vector (already initialized)\n",
    "#             pass\n",
    "\n",
    "#     # Set up device (GPU if available, otherwise CPU)\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     # Load the normalized SAE model\n",
    "#     sae = load_sae(model_path=sae_dir / \"ae_normalized.pt\", device=device)\n",
    "\n",
    "#     # Process the shard and get results (true positives, false positives, and true positives per domain)\n",
    "#     (tp, fp, fn, tp_per_domain) = process_shard(\n",
    "#         sae,\n",
    "#         device,\n",
    "#         esm_embds_dir / f\"{shard}\" / \"activations.pt\",\n",
    "#         per_token_labels,\n",
    "#         threshold_percents,\n",
    "#         concept_names,\n",
    "#         feat_chunk_max=250,\n",
    "#         is_sparse=is_sparse,\n",
    "#     )\n",
    "\n",
    "#     # Create output directory if it doesn't exist and save results\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     np.savez_compressed(\n",
    "#         output_dir / f\"{shard}_counts.npz\",\n",
    "#         tp=tp,\n",
    "#         fp=fp,\n",
    "#         fn=fn,\n",
    "#         tp_per_domain=tp_per_domain,\n",
    "#     )\n",
    "\n",
    "#     # Save per_token_labels to npz in f\"shard_{i}/aa_concepts.npz\"\n",
    "#     shard_dir = output_dir / f\"{shard}\"\n",
    "#     shard_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # Convert to sparse matrix (recommended if data is mostly zeros)\n",
    "#     per_token_labels_matrix = sparse.csr_matrix(per_token_labels)\n",
    "#     sparse.save_npz(shard_dir / \"gene_concepts.npz\", per_token_labels_matrix)\n",
    "\n",
    "\n",
    "def analyze_concepts(\n",
    "    adata_path: Path,\n",
    "    gene_ids_path: Path,\n",
    "    concepts_path: Path,\n",
    "    gene_ignore: List,\n",
    "    sae_dir: Path,\n",
    "    esm_embds_dir: Path = Path(\"../../data/processed/embeddings\"),\n",
    "    eval_set_dir: Path = Path(\"../../data/processed/valid\"),\n",
    "    output_dir: Path = Path(\"concept_results\"),\n",
    "    threshold_percents: List[float] = [0, 0.15, 0.5, 0.6, 0.8],\n",
    "    shard: Optional[str] = None,  # e.g., 'shard_55'\n",
    "    is_sparse: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze concepts using an SAE for one shard.\n",
    "\n",
    "    Saves NPZ:\n",
    "      - tp, fp, fn: [C, F, T]\n",
    "      - tp_per_domain: [C, F, T] (placeholder zeros)\n",
    "      - gene_concepts.npz: sparse CSR of per-token labels (N x C)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Preconditions ---\n",
    "    if not (sae_dir / \"ae_normalized.pt\").exists():\n",
    "        raise ValueError(f\"Normalized SAE model not found in {sae_dir}\")\n",
    "\n",
    "    # Load AnnData (not used for N-alignment; keep as-is for any side checks you need)\n",
    "    ad_ = sc.read_h5ad(adata_path)\n",
    "    ad_subset = ad_[ad_.obs[\"shards\"] == shard].copy()\n",
    "    if \"index\" in ad_subset.var.columns:\n",
    "        genes_to_keep = ~ad_subset.var[\"index\"].isin(gene_ignore)\n",
    "    else:\n",
    "        genes_to_keep = np.ones(ad_subset.var.shape[0], dtype=bool)\n",
    "    ad_subset = ad_subset[:, genes_to_keep]\n",
    "\n",
    "    # --- Load (cell, token) pairs and keep only the token column for labels ---\n",
    "    pair_path = gene_ids_path / shard / \"cell_gene_pairs.txt\"\n",
    "    if not pair_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {pair_path}\")\n",
    "\n",
    "    toks = (pair_path.read_text()).split()\n",
    "    if len(toks) % 2 != 0:\n",
    "        raise ValueError(\n",
    "            f\"{shard}: cell_gene_pairs.txt has odd token count ({len(toks)}). \"\n",
    "            \"Expected (cell, token) pairs.\"\n",
    "        )\n",
    "\n",
    "    # (cell, token) pairs -> take only tokens for label rows\n",
    "    pairs = list(zip(toks[0::2], toks[1::2]))\n",
    "    _, tokens = zip(*pairs)  # tokens length == N expected for activations\n",
    "\n",
    "    # --- Load concepts and build token->label rows ---\n",
    "    df = pd.read_csv(concepts_path / \"gene_concepts.csv\", index_col=0)\n",
    "    gene_to_concepts = {gene: df[gene].to_numpy(dtype=np.float32) for gene in df.columns}\n",
    "\n",
    "    concept_names = load_concept_names(concepts_path / \"gprofiler_gene_concepts_columns.txt\")\n",
    "    C = len(concept_names)\n",
    "    per_token_labels = np.zeros((len(tokens), C), dtype=np.float32)\n",
    "\n",
    "    # special tokens to zero out (no label)\n",
    "    special_tokens = {\"<cls>\", \"<pad>\", \"<unk>\", \"[CLS]\", \"[PAD]\", \"[UNK]\"}\n",
    "    ignore_set = set(gene_ignore) if gene_ignore is not None else set()\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        # keep row but zero it out for specials / ignored genes to maintain N alignment\n",
    "        if tok in special_tokens or tok in ignore_set:\n",
    "            continue\n",
    "        row = gene_to_concepts.get(tok)\n",
    "        if row is not None:\n",
    "            per_token_labels[i] = row  # shape (C,)\n",
    "\n",
    "    # --- Device & model ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sae = load_sae(model_path=sae_dir / \"ae_normalized.pt\", device=device)\n",
    "\n",
    "    # --- OPTIONAL sanity check for N against activations ---\n",
    "    acts_pt = esm_embds_dir / f\"{shard}\" / \"activations.pt\"\n",
    "    if not acts_pt.exists():\n",
    "        raise FileNotFoundError(f\"Missing activations at {acts_pt}\")\n",
    "    # Only to peek at N; process_shard will load again to the right device\n",
    "    acts_preview = torch.load(acts_pt, map_location=\"cpu\", weights_only=True)\n",
    "    if isinstance(acts_preview, torch.Tensor):\n",
    "        N_acts = acts_preview.shape[0]\n",
    "    elif isinstance(acts_preview, dict):\n",
    "        # try common key or first tensor value\n",
    "        if \"activations\" in acts_preview and isinstance(acts_preview[\"activations\"], torch.Tensor):\n",
    "            N_acts = acts_preview[\"activations\"].shape[0]\n",
    "        else:\n",
    "            N_acts = next(v for v in acts_preview.values() if isinstance(v, torch.Tensor)).shape[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized activations format at {acts_pt}\")\n",
    "\n",
    "    if len(tokens) != N_acts:\n",
    "        raise RuntimeError(\n",
    "            f\"N mismatch for {shard}: tokens={len(tokens)} vs activations={N_acts}. \"\n",
    "            \"The token list must correspond exactly to the activations generation.\"\n",
    "        )\n",
    "\n",
    "    print(f\"[check] shard={shard}  N={len(tokens)}  C={C}\")\n",
    "\n",
    "    # --- Run metrics over feature chunks ---\n",
    "    (tp, fp, fn, tp_per_domain) = process_shard(\n",
    "        sae=sae,\n",
    "        device=device,\n",
    "        esm_embeddings_pt_path=str(acts_pt),\n",
    "        per_token_labels=per_token_labels,\n",
    "        threshold_percents=threshold_percents,\n",
    "        is_aa_concept_list=[False] * C,  # placeholder; not used in current dense path\n",
    "        feat_chunk_max=250,\n",
    "        is_sparse=is_sparse,\n",
    "    )\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        output_dir / f\"{shard}_counts.npz\",\n",
    "        tp=tp,\n",
    "        fp=fp,\n",
    "        fn=fn,\n",
    "        tp_per_domain=tp_per_domain,\n",
    "    )\n",
    "\n",
    "    shard_dir = output_dir / f\"{shard}\"\n",
    "    shard_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save per-token labels (sparse) for later inspection\n",
    "    per_token_labels_matrix = sparse.csr_matrix(per_token_labels)\n",
    "    sparse.save_npz(shard_dir / \"gene_concepts.npz\", per_token_labels_matrix)\n",
    "\n",
    "\n",
    "\n",
    "def analyze_all_shards_in_set(\n",
    "    adata_path: Path,\n",
    "    sae_dir: Path,\n",
    "    embds_dir: Path,\n",
    "    concepts_path: Path,\n",
    "    eval_set_dir: Path,\n",
    "    gene_ids_path: Path,\n",
    "    gene_ignore: List,\n",
    "    output_dir: Path = \"concept_results\",\n",
    "    threshold_percents: List[float] = [0, 0.15, 0.5, 0.6, 0.8],\n",
    "    is_sparse: bool = True,\n",
    "):\n",
    "    \"\"\"Wrapper to scan calculate metrics across all shards in an evaluation set.\n",
    "\n",
    "    Args:\n",
    "        sae_dir (Path): Directory containing the normalized SAE model file 'ae_normalized.pt'\n",
    "        embds_dir (Path): Directory containing ESM embeddings\n",
    "        eval_set_dir (Path): Directory containing validation dataset and metadata\n",
    "        output_dir (Path, optional): Directory where results will be saved.\n",
    "        threshold_percents (List[float], optional): List of threshold values for concept detection.\n",
    "        is_sparse (bool, optional): Whether to use sparse matrix operations.\n",
    "\n",
    "    Returns:\n",
    "        None: Results for each shard are saved to disk in the output_dir\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If metadata.json is not found in eval_set_dir\n",
    "        ValueError: If any individual shard analysis fails (inherited from analyze_concepts)\n",
    "    \"\"\"\n",
    "    # Load list of shards to evaluate from metadata\n",
    "    print(eval_set_dir)\n",
    "    # with open(eval_set_dir / \"metadata.json\", \"r\") as f:\n",
    "    shards_to_eval = os.listdir(eval_set_dir)\n",
    "    print(f\"Analyzing set {eval_set_dir.stem} with {shards_to_eval} shards\")\n",
    "\n",
    "    # Process each shard sequentially\n",
    "    for shard in shards_to_eval:\n",
    "        analyze_concepts(\n",
    "            adata_path,\n",
    "            gene_ids_path,\n",
    "            concepts_path,\n",
    "            gene_ignore,\n",
    "            sae_dir,\n",
    "            embds_dir,\n",
    "            eval_set_dir,\n",
    "            output_dir,\n",
    "            threshold_percents,\n",
    "            shard,\n",
    "            is_sparse,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab15b19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/activations/layer_4_test\n",
      "Analyzing set layer_4_test with ['shard_55', 'shard_57', 'shard_56', 'shard_58', 'shard_59'] shards\n",
      "[check] shard=shard_55  N=511688  C=2885\n",
      "Calculating over 4096 features in 17 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:25<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] shard=shard_57  N=530938  C=2885\n",
      "Calculating over 4096 features in 17 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:25<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] shard=shard_56  N=485231  C=2885\n",
      "Calculating over 4096 features in 17 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:23<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] shard=shard_58  N=521344  C=2885\n",
      "Calculating over 4096 features in 17 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:50<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] shard=shard_59  N=521464  C=2885\n",
      "Calculating over 4096 features in 17 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:25<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "sae_dir=Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/sae_latents/sae_output_layer4')\n",
    "embds_dir=Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/activations/layer_4')\n",
    "eval_set_dir=Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/activations/layer_4_test')\n",
    "output_dir=Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output')\n",
    "gene_ids_path = Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/gene_ids')\n",
    "concepts_path = Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/gprofiler_annotation')\n",
    "\n",
    "adata_path = Path('/maiziezhou_lab2/yunfei/Projects/FM_temp/InterPLM/interplm/ge_shards/cosmx_human_lung_sec8.h5ad')\n",
    "\n",
    "filtered_genes = ['RGS5', 'CCL3L3']\n",
    "\n",
    "analyze_all_shards_in_set(\n",
    "        adata_path=adata_path,\n",
    "        sae_dir=sae_dir,\n",
    "        embds_dir=embds_dir,\n",
    "        concepts_path=concepts_path,\n",
    "        eval_set_dir=eval_set_dir,\n",
    "        output_dir=output_dir,\n",
    "        gene_ids_path=gene_ids_path,\n",
    "        gene_ignore=filtered_genes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f91b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68356046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> CHECK 1: Are you iterating over actual shard directories only?\n",
      "\n",
      "Shards under /maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/activations/layer_4_test: 5\n",
      "first 10: ['shard_55', 'shard_57', 'shard_56', 'shard_58', 'shard_59']\n",
      "\n",
      "Shards under /maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/activations/layer_4: 60\n",
      "first 10: ['shard_18', 'shard_28', 'shard_40', 'shard_34', 'shard_55', 'shard_12', 'shard_29', 'shard_1', 'shard_54', 'shard_46']\n",
      "\n",
      "Shards under /maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/gene_ids: 60\n",
      "first 10: ['shard_18', 'shard_28', 'shard_40', 'shard_34', 'shard_55', 'shard_12', 'shard_29', 'shard_1', 'shard_54', 'shard_46']\n",
      "\n",
      ">>> CHECK 2: Intersections and mismatches\n",
      "eval ∩ embds: 5\n",
      "eval ∩ gene_ids: 5\n",
      "embds ∩ gene_ids: 60\n",
      "Missing in embds: []\n",
      "Missing in gene_ids: []\n",
      "\n",
      ">>> CHECK 3: Inspect a few shards in detail\n",
      "\n",
      "=== SHARD: shard_55 ===\n",
      "activations.pt exists: True\n",
      "cell_gene_pairs.txt exists: True\n",
      "N_activations: 511688   full shape: torch.Size([511688, 512])\n",
      "N_gene_ids: 1023376\n",
      "unique gene_ids: 2296   duplicates: 1021080\n",
      "first 5 gene_ids: ['140_1-7', '<cls>', '140_1-7', 'ACVR1', '140_1-7']\n",
      "\n",
      "=== SHARD: shard_56 ===\n",
      "activations.pt exists: True\n",
      "cell_gene_pairs.txt exists: True\n",
      "N_activations: 485231   full shape: torch.Size([485231, 512])\n",
      "N_gene_ids: 970462\n",
      "unique gene_ids: 2331   duplicates: 968131\n",
      "first 5 gene_ids: ['226_1-7', '<cls>', '226_1-7', 'ADGRE5', '226_1-7']\n",
      "\n",
      "=== SHARD: shard_57 ===\n",
      "activations.pt exists: True\n",
      "cell_gene_pairs.txt exists: True\n",
      "N_activations: 530938   full shape: torch.Size([530938, 512])\n",
      "N_gene_ids: 1061876\n",
      "unique gene_ids: 2322   duplicates: 1059554\n",
      "first 5 gene_ids: ['111_1-7', '<cls>', '111_1-7', 'ACKR3', '111_1-7']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, json, torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def infer_N_from_activations(acts):\n",
    "    \"\"\"Infer N (#examples/tokens) from an activations.pt structure.\"\"\"\n",
    "    if isinstance(acts, torch.Tensor):\n",
    "        return acts.shape[0], acts.shape\n",
    "    if isinstance(acts, dict):\n",
    "        # try common keys / first tensor\n",
    "        for k in ['activations', 'acts', 'X', 'z', 'hidden_states']:\n",
    "            if k in acts and isinstance(acts[k], torch.Tensor):\n",
    "                t = acts[k]\n",
    "                return t.shape[0], t.shape\n",
    "        for v in acts.values():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                return v.shape[0], v.shape\n",
    "    raise ValueError(\"Cannot infer N from activations structure\")\n",
    "\n",
    "def read_gene_ids(gene_ids_file: Path):\n",
    "    with open(gene_ids_file, \"r\") as f:\n",
    "        ids = f.read().split()\n",
    "    return ids\n",
    "\n",
    "def show_shard_alignment(shard: str,\n",
    "                         embds_dir: Path,\n",
    "                         gene_ids_path: Path):\n",
    "    print(f\"\\n=== SHARD: {shard} ===\")\n",
    "    activations_file = embds_dir / shard / \"activations.pt\"\n",
    "    gene_ids_file    = gene_ids_path / shard / \"cell_gene_pairs.txt\"\n",
    "\n",
    "    print(\"activations.pt exists:\", activations_file.exists())\n",
    "    print(\"cell_gene_pairs.txt exists:\", gene_ids_file.exists())\n",
    "\n",
    "    if activations_file.exists():\n",
    "        acts = torch.load(activations_file, map_location=\"cpu\")\n",
    "        try:\n",
    "            N_acts, shape_acts = infer_N_from_activations(acts)\n",
    "            print(\"N_activations:\", N_acts, \"  full shape:\", shape_acts)\n",
    "        except Exception as e:\n",
    "            print(\"Could not infer N from activations:\", repr(e))\n",
    "    else:\n",
    "        print(\"Missing activations file.\")\n",
    "\n",
    "    if gene_ids_file.exists():\n",
    "        ids = read_gene_ids(gene_ids_file)\n",
    "        print(\"N_gene_ids:\", len(ids))\n",
    "        # Quick duplicate/format check\n",
    "        n_unique = len(set(ids))\n",
    "        print(\"unique gene_ids:\", n_unique, \"  duplicates:\", len(ids) - n_unique)\n",
    "        if len(ids) > 0:\n",
    "            print(\"first 5 gene_ids:\", ids[:5])\n",
    "            # If entries look like \"cell,gene\" pairs, show a split preview\n",
    "            if \",\" in ids[0] or \"\\t\" in ids[0]:\n",
    "                delim = \",\" if \",\" in ids[0] else \"\\t\"\n",
    "                preview = [x.split(delim)[:2] for x in ids[:5]]\n",
    "                print(\"first 5 parsed pairs:\", preview)\n",
    "    else:\n",
    "        print(\"Missing gene_ids file.\")\n",
    "\n",
    "def list_shards_clean(dir_path: Path):\n",
    "    \"\"\"List shard names but only include directories (avoid files like .DS_Store).\"\"\"\n",
    "    items = [d for d in os.listdir(dir_path) if (dir_path / d).is_dir()]\n",
    "    print(f\"\\nShards under {dir_path}: {len(items)}\")\n",
    "    if items:\n",
    "        print(\"first 10:\", items[:10])\n",
    "    return items\n",
    "\n",
    "# ---- Run the checks ----\n",
    "print(\">>> CHECK 1: Are you iterating over actual shard directories only?\")\n",
    "shards_eval = list_shards_clean(eval_set_dir)\n",
    "shards_emb  = list_shards_clean(embds_dir)\n",
    "shards_ids  = list_shards_clean(gene_ids_path)\n",
    "\n",
    "print(\"\\n>>> CHECK 2: Intersections and mismatches\")\n",
    "S_eval, S_emb, S_ids = set(shards_eval), set(shards_emb), set(shards_ids)\n",
    "print(\"eval ∩ embds:\", len(S_eval & S_emb))\n",
    "print(\"eval ∩ gene_ids:\", len(S_eval & S_ids))\n",
    "print(\"embds ∩ gene_ids:\", len(S_emb & S_ids))\n",
    "print(\"Missing in embds:\", sorted(S_eval - S_emb)[:10])\n",
    "print(\"Missing in gene_ids:\", sorted(S_eval - S_ids)[:10])\n",
    "\n",
    "print(\"\\n>>> CHECK 3: Inspect a few shards in detail\")\n",
    "for s in sorted((S_eval & S_emb & S_ids))[:3]:  # inspect up to 3 shards\n",
    "    show_shard_alignment(s, embds_dir, gene_ids_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
