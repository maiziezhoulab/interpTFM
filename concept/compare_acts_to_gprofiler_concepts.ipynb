{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84b5842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Optional\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('/maiziezhou_lab2/yunfei/Projects/interpTFM/sae')\n",
    "from dictionary import AutoEncoder\n",
    "from inference import get_sae_feats_in_batches, load_sae\n",
    "\n",
    "\n",
    "def process_shard_activations(\n",
    "    device: torch.device,\n",
    "    esm_embeddings_pt_path: str,\n",
    "    per_token_labels: Union[np.ndarray, \"sparse.spmatrix\"],\n",
    "    threshold_percents: List[float],\n",
    "    is_aa_concept_list: List[bool],\n",
    "    feat_chunk_max: int = 512,\n",
    "    is_sparse: bool = False,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Process a shard by computing metrics for raw activation features (no SAE).\n",
    "    Each activation dimension is treated as a 'feature'.\n",
    "    \"\"\"\n",
    "    # 1) Load activations\n",
    "    acts = torch.load(\n",
    "        esm_embeddings_pt_path, map_location=device, weights_only=True\n",
    "    )\n",
    "    # Expected shape: [n_tokens_or_genes, n_features]\n",
    "    if not torch.is_tensor(acts):\n",
    "        acts = torch.tensor(acts, device=device)\n",
    "    else:\n",
    "        acts = acts.to(device)\n",
    "\n",
    "    # 2) Chunking over feature dimensions (columns)\n",
    "    total_features = acts.shape[1]\n",
    "    feature_chunk_size = min(feat_chunk_max, total_features)\n",
    "    num_chunks = int(np.ceil(total_features / feature_chunk_size))\n",
    "    print(f\"Calculating over {total_features} activation features in {num_chunks} chunks\")\n",
    "\n",
    "    # 3) Allocate outputs\n",
    "    n_concepts = per_token_labels.shape[1]\n",
    "    n_thresholds = len(threshold_percents)\n",
    "    tp = np.zeros((n_concepts, total_features, n_thresholds))\n",
    "    fp = np.zeros((n_concepts, total_features, n_thresholds))\n",
    "    fn = np.zeros((n_concepts, total_features, n_thresholds))\n",
    "    tp_per_domain = np.zeros((n_concepts, total_features, n_thresholds))\n",
    "\n",
    "    # 4) Ensure labels are dense np.ndarray if calc_metrics_dense expects dense\n",
    "    # If your calc_metrics_dense supports torch tensors, you can skip this conversion.\n",
    "    # per_token_labels_arr = per_token_labels.toarray() if hasattr(per_token_labels, \"toarray\") else per_token_labels\n",
    "    per_token_labels_arr = per_token_labels\n",
    "\n",
    "    # 5) Loop over feature chunks; no SAE encoding, we just slice columns\n",
    "    for feature_list in tqdm(np.array_split(range(total_features), num_chunks)):\n",
    "        # feature_list is a 1D array of column indices\n",
    "        feats_subset = acts[:, feature_list]  # shape: [N, |subset|]\n",
    "\n",
    "        # If calc_metrics_dense expects numpy, convert here:\n",
    "        # feats_subset_np = feats_subset.detach().cpu().numpy()\n",
    "        feats_subset_np = feats_subset  # keep as tensor if your calc handles torch\n",
    "\n",
    "        metrics = calc_metrics_dense(\n",
    "            feats_subset_np, per_token_labels_arr, threshold_percents, is_aa_concept_list\n",
    "        )\n",
    "        tp_subset, fp_subset, fn_subset, tp_per_domain_subset = metrics\n",
    "\n",
    "        tp[:, feature_list] = tp_subset\n",
    "        fp[:, feature_list] = fp_subset\n",
    "        fn[:, feature_list] = fn_subset\n",
    "        tp_per_domain[:, feature_list] = tp_per_domain_subset\n",
    "\n",
    "    return (tp, fp, fn, tp_per_domain)\n",
    "\n",
    "def analyze_concepts_with_activations(\n",
    "    adata_path: Path,\n",
    "    gene_ids_path: Path,\n",
    "    concepts_path: Path,\n",
    "    gene_ignore: List,\n",
    "    esm_embds_dir: Path,\n",
    "    eval_set_dir: Path = Path(\"../../data/processed/valid\"),\n",
    "    output_dir: Path = Path(\"concept_results_scgpt_acts\"),\n",
    "    threshold_percents: List[float] = [0, 0.15, 0.5, 0.6, 0.8],\n",
    "    shard: Optional[str] = None,  # e.g., 'shard_55'\n",
    "    is_sparse: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Associate gene-level concepts with raw scGPT activations (no SAE).\n",
    "    Saves tp/fp/fn/tp_per_domain for apples-to-apples plots with SAE pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Load AnnData (optional filtering, does NOT affect N alignment) ---\n",
    "    ad_ = sc.read_h5ad(adata_path)\n",
    "    ad_subset = ad_[ad_.obs[\"shards\"] == shard].copy()\n",
    "    if \"index\" in ad_subset.var.columns:\n",
    "        genes_to_keep = ~ad_subset.var[\"index\"].isin(gene_ignore)\n",
    "        ad_subset = ad_subset[:, genes_to_keep]\n",
    "\n",
    "    # --- Parse cell/gene pairs: keep only the token column ---\n",
    "    pair_path = gene_ids_path / shard / \"cell_gene_pairs.txt\"\n",
    "    if not pair_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {pair_path}\")\n",
    "\n",
    "    toks = (pair_path.read_text()).split()\n",
    "    if len(toks) % 2 != 0:\n",
    "        raise ValueError(\n",
    "            f\"{shard}: cell_gene_pairs.txt has odd token count ({len(toks)}). \"\n",
    "            \"Expected (cell, token) pairs.\"\n",
    "        )\n",
    "    pairs = list(zip(toks[0::2], toks[1::2]))\n",
    "    _, tokens = zip(*pairs)  # tokens length should match activations N\n",
    "\n",
    "    # --- Load concepts and build per-token label matrix ---\n",
    "    df = pd.read_csv(concepts_path / \"gene_concepts.csv\", index_col=0)\n",
    "    # Dict: gene -> vector (C,)\n",
    "    gene_to_concepts = {gene: df[gene].to_numpy(dtype=np.float32) for gene in df.columns}\n",
    "\n",
    "    concept_names = load_concept_names(concepts_path / \"gprofiler_gene_concepts_columns.txt\")\n",
    "    C = len(concept_names)\n",
    "\n",
    "    special_tokens = {\"<cls>\", \"<pad>\", \"<unk>\", \"[CLS]\", \"[PAD]\", \"[UNK]\"}\n",
    "    ignore_set = set(gene_ignore) if gene_ignore is not None else set()\n",
    "\n",
    "    per_token_labels = np.zeros((len(tokens), C), dtype=np.float32)\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok in special_tokens or tok in ignore_set:\n",
    "            continue  # leave zero row to preserve alignment\n",
    "        row = gene_to_concepts.get(tok)\n",
    "        if row is not None:\n",
    "            per_token_labels[i] = row\n",
    "\n",
    "    # --- Load activations just to verify N alignment ---\n",
    "    acts_pt = esm_embds_dir / f\"{shard}\" / \"activations.pt\"\n",
    "    if not acts_pt.exists():\n",
    "        raise FileNotFoundError(f\"Missing activations at {acts_pt}\")\n",
    "\n",
    "    # Try to load on CPU first for shape check\n",
    "    try:\n",
    "        acts_preview = torch.load(acts_pt, map_location=\"cpu\", weights_only=True)\n",
    "    except TypeError:\n",
    "        # Older PyTorch doesn't support weights_only\n",
    "        acts_preview = torch.load(acts_pt, map_location=\"cpu\")\n",
    "\n",
    "    if isinstance(acts_preview, torch.Tensor):\n",
    "        N_acts = acts_preview.shape[0]\n",
    "        F = acts_preview.shape[1] if acts_preview.ndim >= 2 else None\n",
    "    elif isinstance(acts_preview, dict):\n",
    "        # Try common key or first tensor value\n",
    "        if \"activations\" in acts_preview and isinstance(acts_preview[\"activations\"], torch.Tensor):\n",
    "            N_acts = acts_preview[\"activations\"].shape[0]\n",
    "            F = acts_preview[\"activations\"].shape[1] if acts_preview[\"activations\"].ndim >= 2 else None\n",
    "        else:\n",
    "            first_tensor = next(v for v in acts_preview.values() if isinstance(v, torch.Tensor))\n",
    "            N_acts = first_tensor.shape[0]\n",
    "            F = first_tensor.shape[1] if first_tensor.ndim >= 2 else None\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized activations format at {acts_pt}\")\n",
    "\n",
    "    if len(tokens) != N_acts:\n",
    "        raise RuntimeError(\n",
    "            f\"N mismatch for {shard}: tokens={len(tokens)} vs activations={N_acts}. \"\n",
    "            \"Ensure the token list matches the activations generation.\"\n",
    "        )\n",
    "\n",
    "    print(f\"[check] shard={shard}  N={len(tokens)}  C={C}  F={F}\")\n",
    "\n",
    "    # --- Choose device (keep your cuda:2 preference, fallback to cpu) ---\n",
    "    device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- Compute metrics directly on activations ---\n",
    "    # NOTE: process_shard_activations is assumed to:\n",
    "    #   - load activations from `esm_embeddings_pt_path`\n",
    "    #   - chunk features\n",
    "    #   - call calc_metrics_dense under the hood\n",
    "    is_aa_concept_list = [False] * C  # placeholder flags; not used in current dense path\n",
    "    (tp, fp, fn, tp_per_domain) = process_shard_activations(\n",
    "        device=device,\n",
    "        esm_embeddings_pt_path=acts_pt,\n",
    "        per_token_labels=per_token_labels,\n",
    "        threshold_percents=threshold_percents,\n",
    "        is_aa_concept_list=is_aa_concept_list,\n",
    "        feat_chunk_max=250,\n",
    "        is_sparse=is_sparse,\n",
    "    )\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        output_dir / f\"{shard}_counts.npz\",\n",
    "        tp=tp, fp=fp, fn=fn, tp_per_domain=tp_per_domain,\n",
    "    )\n",
    "\n",
    "    shard_dir = output_dir / f\"{shard}\"\n",
    "    shard_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save labels (sparse recommended)\n",
    "    per_token_labels_matrix = sparse.csr_matrix(per_token_labels)\n",
    "    sparse.save_npz(shard_dir / \"gene_concepts.npz\", per_token_labels_matrix)\n",
    "\n",
    "    # Save concept names for later reference\n",
    "    with open(shard_dir / \"concept_names.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(concept_names))\n",
    "\n",
    "\n",
    "def analyze_all_shards_with_activations(\n",
    "    adata_path: Path,\n",
    "    embds_dir: Path,\n",
    "    concepts_path: Path,\n",
    "    eval_set_dir: Path,\n",
    "    gene_ids_path: Path,\n",
    "    gene_ignore: List,\n",
    "    output_dir: Path = Path(\"concept_results_scgpt_acts\"),\n",
    "    threshold_percents: List[float] = [0, 0.15, 0.5, 0.6, 0.8],\n",
    "    is_sparse: bool = True,\n",
    "):\n",
    "    shards_to_eval = os.listdir(eval_set_dir)\n",
    "    print(f\"Analyzing set {eval_set_dir.stem} with {len(shards_to_eval)} shards\")\n",
    "\n",
    "    for shard in shards_to_eval:\n",
    "        analyze_concepts_with_activations(\n",
    "            adata_path=adata_path,\n",
    "            gene_ids_path=gene_ids_path,\n",
    "            concepts_path=concepts_path,\n",
    "            gene_ignore=gene_ignore,\n",
    "            esm_embds_dir=embds_dir,\n",
    "            eval_set_dir=eval_set_dir,\n",
    "            output_dir=output_dir,\n",
    "            threshold_percents=threshold_percents,\n",
    "            shard=shard,\n",
    "            is_sparse=is_sparse,\n",
    "        )\n",
    "\n",
    "def load_concept_names(concept_name_path: Path) -> List[str]:\n",
    "    \"\"\"Load concept names from a file.\"\"\"\n",
    "    with open(concept_name_path, \"r\") as f:\n",
    "        return f.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a8b7be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_nonzero_dense(matrix: torch.Tensor) -> List[int]:\n",
    "    \"\"\"\n",
    "    Count unique non-zero values in each column of a dense matrix.\n",
    "\n",
    "    Args:\n",
    "        matrix: Dense PyTorch tensor to analyze\n",
    "\n",
    "    Returns:\n",
    "        List of counts of unique non-zero values for each column\n",
    "    \"\"\"\n",
    "    # Initialize list to store counts\n",
    "    unique_counts = []\n",
    "\n",
    "    # Iterate through each column\n",
    "    for col in range(matrix.shape[1]):\n",
    "        # Get unique values in the column\n",
    "        unique_values = torch.unique(matrix[:, col])\n",
    "        # Count how many unique values are non-zero\n",
    "        count = torch.sum(unique_values != 0).item()\n",
    "        unique_counts.append(count)\n",
    "\n",
    "    return unique_counts\n",
    "\n",
    "\n",
    "def calc_metrics_dense(\n",
    "    sae_feats: torch.Tensor,\n",
    "    per_token_labels_sparse: Union[np.ndarray, sparse.spmatrix],\n",
    "    threshold_percents: List[float],\n",
    "    is_aa_level_concept: List[bool],\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Optimized GPU-compatible metric computation for dense matrices.\n",
    "    \"\"\"\n",
    "    device = sae_feats.device\n",
    "    labels = torch.tensor(per_token_labels_sparse.astype(np.float32), device=device)  # [N, C]\n",
    "    N, F = sae_feats.shape\n",
    "    C = labels.shape[1]\n",
    "    T = len(threshold_percents)\n",
    "\n",
    "    # Thresholds as tensor [T, 1, 1] for broadcasting\n",
    "    thresholds = torch.tensor(threshold_percents, dtype=torch.float32, device=device).view(T, 1, 1)\n",
    "\n",
    "    # Expand and binarize features: [T, N, F]\n",
    "    feats_exp = sae_feats.unsqueeze(0)  # [1, N, F]\n",
    "    bin_feats = (feats_exp > thresholds).float()  # [T, N, F]\n",
    "\n",
    "    # Labels: [1, N, C]\n",
    "    labels_exp = labels.T.unsqueeze(0)  # [1, C, N]\n",
    "\n",
    "    # Calculate TP: [T, C, F]\n",
    "    tp = torch.matmul(labels_exp, bin_feats)  # [T, C, F]\n",
    "    tp = tp.permute(1, 2, 0).contiguous()  # [C, F, T]\n",
    "\n",
    "    # Calculate FP: [T, C, F]\n",
    "    not_labels_exp = (1.0 - labels.T).unsqueeze(0)  # [1, C, N]\n",
    "    fp = torch.matmul(not_labels_exp, bin_feats)  # [T, C, F]\n",
    "    fp = fp.permute(1, 2, 0).contiguous()  # [C, F, T]\n",
    "\n",
    "    # Calculate TP per domain for non-AA-level only\n",
    "    tp_per_domain = torch.zeros_like(tp)\n",
    "\n",
    "    # non_aa_indices = [i for i, flag in enumerate(is_aa_level_concept) if not flag]\n",
    "    # if non_aa_indices:\n",
    "    #     non_aa_mask = torch.zeros(C, dtype=torch.bool, device=device)\n",
    "    #     non_aa_mask[non_aa_indices] = True\n",
    "\n",
    "    #     # For non-AA concepts: compute domain-level TP (number of examples with ≥1 positive feature)\n",
    "    #     for t_idx in range(T):\n",
    "    #         # For each threshold: binary_feats [N, F], labels [N, C]\n",
    "    #         bf = bin_feats[t_idx]  # [N, F]\n",
    "    #         l = labels  # [N, C]\n",
    "\n",
    "    #         # Multiply elementwise [N, F] * [N, C] -> [N, C, F]\n",
    "    #         combined = (bf.unsqueeze(1) * l.unsqueeze(2))  # [N, C, F]\n",
    "    #         per_domain_tp = (combined.sum(dim=0) > 0).float()  # [C, F]\n",
    "    #         tp_per_domain[:, :, t_idx] = per_domain_tp\n",
    "\n",
    "    positive_labels = labels.sum(dim=0)  # [C]\n",
    "    positive_labels = positive_labels.view(-1, 1, 1)  # [C, 1, 1]\n",
    "\n",
    "    fn = positive_labels - tp  # [C, F, T]\n",
    "    fn = torch.clamp(fn, min=0)  # Optional, to avoid negative values due to float precision\n",
    "\n",
    "    return tp.cpu().numpy(), fp.cpu().numpy(), fn.cpu().numpy(), tp_per_domain.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b50830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing set layer_4_test with 5 shards\n",
      "[check] shard=shard_55  N=511688  C=2885  F=512\n",
      "Calculating over 512 activation features in 3 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] shard=shard_57  N=530938  C=2885  F=512\n",
      "Calculating over 512 activation features in 3 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] shard=shard_56  N=485231  C=2885  F=512\n",
      "Calculating over 512 activation features in 3 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] shard=shard_58  N=521344  C=2885  F=512\n",
      "Calculating over 512 activation features in 3 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] shard=shard_59  N=521464  C=2885  F=512\n",
      "Calculating over 512 activation features in 3 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# sae_dir=Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/sae_latents/sae_output_layer4')\n",
    "embds_dir=Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/activations/layer_4')\n",
    "eval_set_dir=Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/activations/layer_4_test')\n",
    "output_dir=Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/output_acts')\n",
    "gene_ids_path = Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/activations_cosmx_lung_cancer/gene_ids')\n",
    "concepts_path = Path('/maiziezhou_lab2/yunfei/Projects/interpTFM/gprofiler_annotation')\n",
    "\n",
    "adata_path = Path('/maiziezhou_lab2/yunfei/Projects/FM_temp/InterPLM/interplm/ge_shards/cosmx_human_lung_sec8.h5ad')\n",
    "\n",
    "filtered_genes = ['RGS5', 'CCL3L3']\n",
    "\n",
    "analyze_all_shards_with_activations(\n",
    "        adata_path=adata_path,\n",
    "        # sae_dir=sae_dir,\n",
    "        embds_dir=embds_dir,\n",
    "        concepts_path=concepts_path,\n",
    "        eval_set_dir=eval_set_dir,\n",
    "        output_dir=output_dir,\n",
    "        gene_ids_path=gene_ids_path,\n",
    "        gene_ignore=filtered_genes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb01d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7b77d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
