{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d27c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for training SAE from a cache of scGPT activations and loading the custom fidelity function.\n",
    "Assumes specific dataset structure for activations, as created by interp/data_processing/embed_fasta.py.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('/maiziezhou_lab2/yunfei/Projects/FM_temp/InterPLM/interplm')\n",
    "\n",
    "from sae.dictionary import AutoEncoder\n",
    "# from train.fidelity import get_loss_recovery_fn\n",
    "from train.load_sharded_acts import GFMDataset\n",
    "from train.trainer import StandardTrainer\n",
    "from train.training import train_run\n",
    "from utils import get_device\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"TypedStorage is deprecated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aa4ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SAE_on_gfm_embeds(\n",
    "    # Data paths and sources\n",
    "    embd_dir: Path,\n",
    "    eval_seq_path: Path,\n",
    "    layer='',\n",
    "    # Core model architecture\n",
    "    expansion_factor: int = 8,\n",
    "    # Training configuration\n",
    "    batch_size: int = 32,\n",
    "    steps: int = 1_000,\n",
    "    seed: int = 0,\n",
    "    # Optimization parameters\n",
    "    lr: float = 1e-3,\n",
    "    warmup_steps: int = 50,\n",
    "    resample_steps: int = 0,  # 0 to disable\n",
    "    # Regularization\n",
    "    l1_penalty: float = 1e-1,\n",
    "    l1_annealing_pct: float = 0.05,\n",
    "    # Evaluation settings\n",
    "    eval_batch_size: int = 128,\n",
    "    eval_steps: int = 1_000,\n",
    "    # Logging and checkpointing\n",
    "    save_dir: str = \"models\",\n",
    "    log_steps: int = 100,\n",
    "    save_steps: int = 50,\n",
    "    max_ckpts_to_keep: int = 3,\n",
    "    # Weights & Biases configuration\n",
    "    use_wandb: bool = False,\n",
    "    wandb_entity: str = \"\",\n",
    "    wandb_project: str = \"test_logging\",\n",
    "    wandb_name: str = \"SAE\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a Sparse Autoencoder (SAE) using cached activation data from a language model.\n",
    "\n",
    "    Args:\n",
    "        # Data paths and sources\n",
    "        embd_dir: Directory containing cached model embeddings\n",
    "        eval_seq_path: Path to sequences for fidelity evaluation, if None, fidelity evaluation is disabled\n",
    "\n",
    "        # Core model architecture\n",
    "        expansion_factor: Factor by which to expand the dictionary size relative to input dimension\n",
    "\n",
    "        # Training configuration\n",
    "        batch_size: Number of samples per training batch\n",
    "        steps: Total number of training steps\n",
    "        seed: Random seed for reproducibility\n",
    "\n",
    "        # Optimization parameters\n",
    "        lr: Learning rate for optimizer\n",
    "        warmup_steps: Number of warmup steps for learning rate scheduler\n",
    "        resample_steps: Steps between dictionary resampling (0 to disable)\n",
    "\n",
    "        # Regularization\n",
    "        l1_penalty: Coefficient for L1 regularization\n",
    "        l1_annealing_pct: Percentage of training during which to anneal L1 penalty\n",
    "\n",
    "        # Evaluation settings\n",
    "        eval_batch_size: Batch size for evaluation\n",
    "        eval_steps: Frequency of evaluation steps\n",
    "\n",
    "        # Logging and checkpointing\n",
    "        save_dir: Directory to save model checkpoints and outputs\n",
    "        log_steps: Frequency of logging\n",
    "        save_steps: Frequency of saving checkpoints\n",
    "\n",
    "        # Weights & Biases configuration\n",
    "        use_wandb: Whether to use Weights & Biases logging\n",
    "        wandb_entity: W&B username or team name\n",
    "        wandb_project: W&B project name\n",
    "        wandb_name: W&B run name\n",
    "    \"\"\"\n",
    "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device='cuda:2'\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return torch.stack(batch).to(device)\n",
    "\n",
    "    # Initialize dataset and dataloader\n",
    "    acts_dataset = GFMDataset(embd_dir)\n",
    "\n",
    "    # Determine layer from dataset metadata\n",
    "    # layer = acts_dataset.datasets[0][\"layer\"]\n",
    "    # plm_name = acts_dataset.datasets[0][\"plm_name\"]\n",
    "    # print(f\"Using activations from layer {layer} of {plm_name}\")\n",
    "    \n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        acts_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "    print(f\"Loaded dataset with {len(acts_dataset):,} tokens\")\n",
    "\n",
    "    # Configure resampling\n",
    "    if resample_steps == 0:\n",
    "        resample_steps = None\n",
    "\n",
    "    # Setup trainer configuration\n",
    "    trainer = StandardTrainer(\n",
    "        activation_dim=acts_dataset.d_model,\n",
    "        dict_size=acts_dataset.d_model * expansion_factor,\n",
    "        warmup_steps=warmup_steps,\n",
    "        resample_steps=resample_steps,\n",
    "        lr=lr,\n",
    "        l1_penalty=l1_penalty,\n",
    "        l1_annealing_pct=l1_annealing_pct,\n",
    "        seed=seed,\n",
    "        wandb_name=wandb_name,\n",
    "        layer=layer,\n",
    "        plm_name='scgpt',\n",
    "        device=device,\n",
    "        steps=min(steps, len(dataloader)),\n",
    "    )\n",
    "    print(f\"Training with config: {trainer.config}\")\n",
    "\n",
    "    # Initialize fidelity function if evaluation sequences provided\n",
    "    if eval_seq_path is not None:\n",
    "        fidelity_fn = get_loss_recovery_fn(\n",
    "            esm_model_name=plm_name,\n",
    "            layer_idx=int(layer),\n",
    "            eval_seq_path=eval_seq_path,\n",
    "            device=device,\n",
    "            batch_size=eval_batch_size,\n",
    "        )\n",
    "    else:\n",
    "        fidelity_fn = None\n",
    "\n",
    "    # Train the SAE\n",
    "    train_run(\n",
    "        # Core training components\n",
    "        data=dataloader,\n",
    "        trainer=trainer,\n",
    "        # Evaluation settings\n",
    "        fidelity_fn=fidelity_fn,\n",
    "        eval_steps=eval_steps,\n",
    "        # Logging and checkpointing\n",
    "        save_dir=save_dir,\n",
    "        log_steps=log_steps,\n",
    "        save_steps=save_steps,\n",
    "        max_ckpts_to_keep=3,\n",
    "        # Weights & Biases configuration\n",
    "        use_wandb=use_wandb,\n",
    "        wandb_entity=wandb_entity,\n",
    "        wandb_project=wandb_project,\n",
    "        additional_wandb_args={\n",
    "            \"eval_seq_path\": eval_seq_path,\n",
    "            \"eval_steps\": eval_steps,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"save_dir\": save_dir,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b767293e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 766/766 [00:00<00:00, 610689.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 766 tokens\n",
      "Training with config: {'dict_class': 'AutoEncoder', 'trainer_class': 'StandardTrainer', 'activation_dim': 512, 'dict_size': 4096, 'lr': 0.001, 'l1_penalty': 0.1, 'l1_annealing_steps': 1, 'steps': 24, 'warmup_steps': 50, 'resample_steps': None, 'device': 'cuda:2', 'layer': 'layer_0', 'plm_name': 'scgpt', 'wandb_name': 'SAE', 'submodule_name': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [264, 512] at entry 0 and [375, 512] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# layer 0\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_SAE_on_gfm_embeds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Data paths and sources\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43membd_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/maiziezhou_lab2/yunfei/Projects/FM_temp/InterPLM/interplm/scgpt/activations/layer_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_seq_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Core model architecture\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpansion_factor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Training configuration\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Optimization parameters\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresample_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 0 to disable\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Regularization\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml1_penalty\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml1_annealing_pct\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Evaluation settings\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Logging and checkpointing\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/maiziezhou_lab2/yunfei/Projects/FM_temp/InterPLM/interplm/scgpt/sae_output/layer_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_ckpts_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Weights & Biases configuration\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_entity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_logging\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSAE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     34\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 128\u001b[0m, in \u001b[0;36mtrain_SAE_on_gfm_embeds\u001b[0;34m(embd_dir, eval_seq_path, layer, expansion_factor, batch_size, steps, seed, lr, warmup_steps, resample_steps, l1_penalty, l1_annealing_pct, eval_batch_size, eval_steps, save_dir, log_steps, save_steps, max_ckpts_to_keep, use_wandb, wandb_entity, wandb_project, wandb_name)\u001b[0m\n\u001b[1;32m    125\u001b[0m     fidelity_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Train the SAE\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m \u001b[43mtrain_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Core training components\u001b[39;49;00m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Evaluation settings\u001b[39;49;00m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfidelity_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfidelity_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Logging and checkpointing\u001b[39;49;00m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_ckpts_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Weights & Biases configuration\u001b[39;49;00m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_entity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_entity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_wandb_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_seq_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_seq_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/maiziezhou_lab2/yunfei/Projects/FM_temp/InterPLM/interplm/train/training.py:97\u001b[0m, in \u001b[0;36mtrain_run\u001b[0;34m(data, trainer, fidelity_fn, eval_steps, save_dir, log_steps, save_steps, max_ckpts_to_keep, use_wandb, wandb_entity, wandb_project, additional_wandb_args)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     96\u001b[0m n_tokens_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, act \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(data, total\u001b[38;5;241m=\u001b[39msteps)):\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m steps:\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped training because reached max specified steps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/scgpt/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scgpt/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/scgpt/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/scgpt/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 77\u001b[0m, in \u001b[0;36mtrain_SAE_on_gfm_embeds.<locals>.collate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_fn\u001b[39m(batch):\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [264, 512] at entry 0 and [375, 512] at entry 1"
     ]
    }
   ],
   "source": [
    "# layer 0\n",
    "\n",
    "train_SAE_on_gfm_embeds(\n",
    "    # Data paths and sources\n",
    "    embd_dir=Path('/maiziezhou_lab2/yunfei/Projects/FM_temp/InterPLM/interplm/scgpt/activations/layer_0'),\n",
    "    eval_seq_path=None,\n",
    "    # Core model architecture\n",
    "    expansion_factor = 8,\n",
    "    layer='layer_0',\n",
    "    # Training configuration\n",
    "    batch_size = 32,\n",
    "    steps = 1_000,\n",
    "    seed = 0,\n",
    "    # Optimization parameters\n",
    "    lr = 1e-3,\n",
    "    warmup_steps = 50,\n",
    "    resample_steps = 0,  # 0 to disable\n",
    "    # Regularization\n",
    "    l1_penalty = 1e-1,\n",
    "    l1_annealing_pct = 0.05,\n",
    "    # Evaluation settings\n",
    "    eval_batch_size = 128,\n",
    "    eval_steps = 1_000,\n",
    "    # Logging and checkpointing\n",
    "    save_dir=Path('/maiziezhou_lab2/yunfei/Projects/FM_temp/InterPLM/interplm/scgpt/sae_output/layer_0'),\n",
    "    log_steps = 100,\n",
    "    save_steps = 50,\n",
    "    max_ckpts_to_keep = 3,\n",
    "    # Weights & Biases configuration\n",
    "    use_wandb = False,\n",
    "    wandb_entity = \"\",\n",
    "    wandb_project = \"test_logging\",\n",
    "    wandb_name = \"SAE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50f72a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "ad = sc.read_h5ad('/maiziezhou_lab2/yunfei/Projects/FM_temp/datasets/cosmx/lung/cosmx_human_lung.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b96d2f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AspectRatio</th>\n",
       "      <th>CenterX_global_px</th>\n",
       "      <th>CenterY_global_px</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Mean.MembraneStain</th>\n",
       "      <th>Max.MembraneStain</th>\n",
       "      <th>Mean.PanCK</th>\n",
       "      <th>Max.PanCK</th>\n",
       "      <th>Mean.CD45</th>\n",
       "      <th>...</th>\n",
       "      <th>assay</th>\n",
       "      <th>organism</th>\n",
       "      <th>sex</th>\n",
       "      <th>tissue</th>\n",
       "      <th>dataset</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>nicheformer_split</th>\n",
       "      <th>_scvi_batch</th>\n",
       "      <th>_scvi_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_1</th>\n",
       "      <td>1.34</td>\n",
       "      <td>4215.888889</td>\n",
       "      <td>158847.666667</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "      <td>3473</td>\n",
       "      <td>7354</td>\n",
       "      <td>715</td>\n",
       "      <td>5755</td>\n",
       "      <td>361</td>\n",
       "      <td>...</td>\n",
       "      <td>NanoString digital spatial profiling</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>female</td>\n",
       "      <td>lung</td>\n",
       "      <td>nanostring_cosmx_human_lung</td>\n",
       "      <td>4215.888889</td>\n",
       "      <td>158847.666667</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_1</th>\n",
       "      <td>1.45</td>\n",
       "      <td>6092.888889</td>\n",
       "      <td>158834.666667</td>\n",
       "      <td>87</td>\n",
       "      <td>60</td>\n",
       "      <td>3895</td>\n",
       "      <td>13832</td>\n",
       "      <td>18374</td>\n",
       "      <td>53158</td>\n",
       "      <td>260</td>\n",
       "      <td>...</td>\n",
       "      <td>NanoString digital spatial profiling</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>female</td>\n",
       "      <td>lung</td>\n",
       "      <td>nanostring_cosmx_human_lung</td>\n",
       "      <td>6092.888889</td>\n",
       "      <td>158834.666667</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1</th>\n",
       "      <td>1.62</td>\n",
       "      <td>7214.888889</td>\n",
       "      <td>158843.666667</td>\n",
       "      <td>68</td>\n",
       "      <td>42</td>\n",
       "      <td>2892</td>\n",
       "      <td>6048</td>\n",
       "      <td>3265</td>\n",
       "      <td>37522</td>\n",
       "      <td>378</td>\n",
       "      <td>...</td>\n",
       "      <td>NanoString digital spatial profiling</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>female</td>\n",
       "      <td>lung</td>\n",
       "      <td>nanostring_cosmx_human_lung</td>\n",
       "      <td>7214.888889</td>\n",
       "      <td>158843.666667</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_1</th>\n",
       "      <td>0.47</td>\n",
       "      <td>7418.888889</td>\n",
       "      <td>158813.666667</td>\n",
       "      <td>48</td>\n",
       "      <td>102</td>\n",
       "      <td>6189</td>\n",
       "      <td>16091</td>\n",
       "      <td>485</td>\n",
       "      <td>964</td>\n",
       "      <td>679</td>\n",
       "      <td>...</td>\n",
       "      <td>NanoString digital spatial profiling</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>female</td>\n",
       "      <td>lung</td>\n",
       "      <td>nanostring_cosmx_human_lung</td>\n",
       "      <td>7418.888889</td>\n",
       "      <td>158813.666667</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>7446.888889</td>\n",
       "      <td>158845.666667</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>8138</td>\n",
       "      <td>19281</td>\n",
       "      <td>549</td>\n",
       "      <td>874</td>\n",
       "      <td>566</td>\n",
       "      <td>...</td>\n",
       "      <td>NanoString digital spatial profiling</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>female</td>\n",
       "      <td>lung</td>\n",
       "      <td>nanostring_cosmx_human_lung</td>\n",
       "      <td>7446.888889</td>\n",
       "      <td>158845.666667</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999_20-2</th>\n",
       "      <td>1.02</td>\n",
       "      <td>-18135.000000</td>\n",
       "      <td>10850.777778</td>\n",
       "      <td>45</td>\n",
       "      <td>44</td>\n",
       "      <td>7430</td>\n",
       "      <td>9572</td>\n",
       "      <td>370</td>\n",
       "      <td>962</td>\n",
       "      <td>450</td>\n",
       "      <td>...</td>\n",
       "      <td>NanoString digital spatial profiling</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>male</td>\n",
       "      <td>lung</td>\n",
       "      <td>nanostring_cosmx_human_lung</td>\n",
       "      <td>-18135.000000</td>\n",
       "      <td>10850.777778</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000_20-2</th>\n",
       "      <td>1.71</td>\n",
       "      <td>-18088.000000</td>\n",
       "      <td>10846.777778</td>\n",
       "      <td>60</td>\n",
       "      <td>35</td>\n",
       "      <td>8362</td>\n",
       "      <td>11209</td>\n",
       "      <td>161</td>\n",
       "      <td>2024</td>\n",
       "      <td>572</td>\n",
       "      <td>...</td>\n",
       "      <td>NanoString digital spatial profiling</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>male</td>\n",
       "      <td>lung</td>\n",
       "      <td>nanostring_cosmx_human_lung</td>\n",
       "      <td>-18088.000000</td>\n",
       "      <td>10846.777778</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001_20-2</th>\n",
       "      <td>2.75</td>\n",
       "      <td>-19112.000000</td>\n",
       "      <td>10846.777778</td>\n",
       "      <td>99</td>\n",
       "      <td>36</td>\n",
       "      <td>5158</td>\n",
       "      <td>10180</td>\n",
       "      <td>634</td>\n",
       "      <td>2166</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>NanoString digital spatial profiling</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>male</td>\n",
       "      <td>lung</td>\n",
       "      <td>nanostring_cosmx_human_lung</td>\n",
       "      <td>-19112.000000</td>\n",
       "      <td>10846.777778</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003_20-2</th>\n",
       "      <td>2.12</td>\n",
       "      <td>-19551.000000</td>\n",
       "      <td>10841.777778</td>\n",
       "      <td>55</td>\n",
       "      <td>26</td>\n",
       "      <td>6339</td>\n",
       "      <td>9804</td>\n",
       "      <td>211</td>\n",
       "      <td>570</td>\n",
       "      <td>488</td>\n",
       "      <td>...</td>\n",
       "      <td>NanoString digital spatial profiling</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>male</td>\n",
       "      <td>lung</td>\n",
       "      <td>nanostring_cosmx_human_lung</td>\n",
       "      <td>-19551.000000</td>\n",
       "      <td>10841.777778</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004_20-2</th>\n",
       "      <td>2.33</td>\n",
       "      <td>-17185.000000</td>\n",
       "      <td>10839.777778</td>\n",
       "      <td>49</td>\n",
       "      <td>21</td>\n",
       "      <td>3491</td>\n",
       "      <td>5600</td>\n",
       "      <td>203</td>\n",
       "      <td>481</td>\n",
       "      <td>335</td>\n",
       "      <td>...</td>\n",
       "      <td>NanoString digital spatial profiling</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>male</td>\n",
       "      <td>lung</td>\n",
       "      <td>nanostring_cosmx_human_lung</td>\n",
       "      <td>-17185.000000</td>\n",
       "      <td>10839.777778</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>771236 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AspectRatio  CenterX_global_px  CenterY_global_px  Width  Height  \\\n",
       "1_1               1.34        4215.888889      158847.666667     47      35   \n",
       "2_1               1.45        6092.888889      158834.666667     87      60   \n",
       "3_1               1.62        7214.888889      158843.666667     68      42   \n",
       "4_1               0.47        7418.888889      158813.666667     48     102   \n",
       "5_1               1.00        7446.888889      158845.666667     38      38   \n",
       "...                ...                ...                ...    ...     ...   \n",
       "3999_20-2         1.02      -18135.000000       10850.777778     45      44   \n",
       "4000_20-2         1.71      -18088.000000       10846.777778     60      35   \n",
       "4001_20-2         2.75      -19112.000000       10846.777778     99      36   \n",
       "4003_20-2         2.12      -19551.000000       10841.777778     55      26   \n",
       "4004_20-2         2.33      -17185.000000       10839.777778     49      21   \n",
       "\n",
       "           Mean.MembraneStain  Max.MembraneStain  Mean.PanCK  Max.PanCK  \\\n",
       "1_1                      3473               7354         715       5755   \n",
       "2_1                      3895              13832       18374      53158   \n",
       "3_1                      2892               6048        3265      37522   \n",
       "4_1                      6189              16091         485        964   \n",
       "5_1                      8138              19281         549        874   \n",
       "...                       ...                ...         ...        ...   \n",
       "3999_20-2                7430               9572         370        962   \n",
       "4000_20-2                8362              11209         161       2024   \n",
       "4001_20-2                5158              10180         634       2166   \n",
       "4003_20-2                6339               9804         211        570   \n",
       "4004_20-2                3491               5600         203        481   \n",
       "\n",
       "           Mean.CD45  ...                                 assay      organism  \\\n",
       "1_1              361  ...  NanoString digital spatial profiling  Homo sapiens   \n",
       "2_1              260  ...  NanoString digital spatial profiling  Homo sapiens   \n",
       "3_1              378  ...  NanoString digital spatial profiling  Homo sapiens   \n",
       "4_1              679  ...  NanoString digital spatial profiling  Homo sapiens   \n",
       "5_1              566  ...  NanoString digital spatial profiling  Homo sapiens   \n",
       "...              ...  ...                                   ...           ...   \n",
       "3999_20-2        450  ...  NanoString digital spatial profiling  Homo sapiens   \n",
       "4000_20-2        572  ...  NanoString digital spatial profiling  Homo sapiens   \n",
       "4001_20-2         41  ...  NanoString digital spatial profiling  Homo sapiens   \n",
       "4003_20-2        488  ...  NanoString digital spatial profiling  Homo sapiens   \n",
       "4004_20-2        335  ...  NanoString digital spatial profiling  Homo sapiens   \n",
       "\n",
       "              sex  tissue                      dataset             x  \\\n",
       "1_1        female    lung  nanostring_cosmx_human_lung   4215.888889   \n",
       "2_1        female    lung  nanostring_cosmx_human_lung   6092.888889   \n",
       "3_1        female    lung  nanostring_cosmx_human_lung   7214.888889   \n",
       "4_1        female    lung  nanostring_cosmx_human_lung   7418.888889   \n",
       "5_1        female    lung  nanostring_cosmx_human_lung   7446.888889   \n",
       "...           ...     ...                          ...           ...   \n",
       "3999_20-2    male    lung  nanostring_cosmx_human_lung -18135.000000   \n",
       "4000_20-2    male    lung  nanostring_cosmx_human_lung -18088.000000   \n",
       "4001_20-2    male    lung  nanostring_cosmx_human_lung -19112.000000   \n",
       "4003_20-2    male    lung  nanostring_cosmx_human_lung -19551.000000   \n",
       "4004_20-2    male    lung  nanostring_cosmx_human_lung -17185.000000   \n",
       "\n",
       "                       y  nicheformer_split _scvi_batch _scvi_labels  \n",
       "1_1        158847.666667              train           0            0  \n",
       "2_1        158834.666667              train           0            0  \n",
       "3_1        158843.666667              train           0            0  \n",
       "4_1        158813.666667              train           0            0  \n",
       "5_1        158845.666667              train           0            0  \n",
       "...                  ...                ...         ...          ...  \n",
       "3999_20-2   10850.777778              train           0            0  \n",
       "4000_20-2   10846.777778              train           0            0  \n",
       "4001_20-2   10846.777778              train           0            0  \n",
       "4003_20-2   10841.777778              train           0            0  \n",
       "4004_20-2   10839.777778              train           0            0  \n",
       "\n",
       "[771236 rows x 40 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "649e6650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the index of the cell by name\n",
    "cell_name = '2359_14-6'\n",
    "cell_idx = ad.obs_names.get_loc(cell_name)\n",
    "\n",
    "# Get the row corresponding to that cell\n",
    "cell_data = ad.X[cell_idx]\n",
    "\n",
    "# If it's sparse, convert to dense first\n",
    "if hasattr(cell_data, 'toarray'):\n",
    "    cell_data = cell_data.toarray().flatten()\n",
    "\n",
    "# Get non-zero values\n",
    "non_zero_values = cell_data[cell_data != 0]\n",
    "\n",
    "print(len(non_zero_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35f2743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.load(\n",
    "            '/maiziezhou_lab2/yunfei/Projects/FM_temp/InterPLM/interplm/scgpt/activations/layer_0/4984_6__T_CD4_naive.pt', map_location=\"cpu\", weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c55ff2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([264, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c0e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3621_17-1_macrophage.pt\n",
      "torch.Size([352, 512])\n",
      "4393_7_fibroblast.pt\n",
      "torch.Size([320, 512])\n",
      "4211_7_plasmablast.pt\n",
      "torch.Size([334, 512])\n",
      "2161_14-1_NK.pt\n",
      "torch.Size([332, 512])\n",
      "1349_15-3_macrophage.pt\n",
      "torch.Size([311, 512])\n",
      "1869_3-5_tumor_9.pt\n",
      "torch.Size([437, 512])\n",
      "999_14-2_epithelial.pt\n",
      "torch.Size([333, 512])\n",
      "256_6-3_tumor_6.pt\n",
      "torch.Size([332, 512])\n",
      "1377_20-1_T_CD4_naive.pt\n",
      "torch.Size([345, 512])\n",
      "1966_11-1_tumor_5.pt\n",
      "torch.Size([408, 512])\n",
      "3419_9_fibroblast.pt\n",
      "torch.Size([264, 512])\n",
      "2083_12-2_tumor_5.pt\n",
      "torch.Size([356, 512])\n",
      "628_3-4_endothelial.pt\n",
      "torch.Size([327, 512])\n",
      "2665_8-2_tumor_5.pt\n",
      "torch.Size([435, 512])\n",
      "384_4-4_endothelial.pt\n",
      "torch.Size([334, 512])\n",
      "224_19-7_T_CD4_memory.pt\n",
      "torch.Size([383, 512])\n",
      "3620_25-1_plasmablast.pt\n",
      "torch.Size([474, 512])\n",
      "875_15-2_NK.pt\n",
      "torch.Size([276, 512])\n",
      "372_9-5_tumor_9.pt\n",
      "torch.Size([474, 512])\n",
      "3092_27-1_endothelial.pt\n",
      "torch.Size([324, 512])\n",
      "1060_19-1_neutrophil.pt\n",
      "torch.Size([438, 512])\n",
      "98_21-5_tumor_12.pt\n",
      "torch.Size([344, 512])\n",
      "976_20-1_fibroblast.pt\n",
      "torch.Size([379, 512])\n",
      "1070_25-3_macrophage.pt\n",
      "torch.Size([397, 512])\n",
      "1290_3-2_tumor_5.pt\n",
      "torch.Size([354, 512])\n",
      "2718_41_epithelial.pt\n",
      "torch.Size([459, 512])\n",
      "1404_4-4_fibroblast.pt\n",
      "torch.Size([350, 512])\n",
      "1525_11-3_tumor_6.pt\n",
      "torch.Size([336, 512])\n",
      "4086_10-1_macrophage.pt\n",
      "torch.Size([352, 512])\n",
      "1578_13-5_T_CD4_naive.pt\n",
      "torch.Size([352, 512])\n",
      "1182_13_neutrophil.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(p)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tensor\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/scgpt/lib/python3.8/site-packages/torch/serialization.py:1007\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/scgpt/lib/python3.8/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/scgpt/lib/python3.8/site-packages/torch/_weights_only_unpickler.py:259\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m     ):\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    257\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m         )\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m    261\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/scgpt/lib/python3.8/site-packages/torch/serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/scgpt/lib/python3.8/site-packages/torch/serialization.py:1357\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1357\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "# dir_ = '/maiziezhou_lab2/yunfei/Projects/FM_temp/InterPLM/interplm/scgpt/activations/layer_0'\n",
    "dir_ = '/maiziezhou_lab2/yunfei/Projects/FM_temp/InterPLM/interplm/scgpt/activations/layer_11'\n",
    "for p in os.listdir(dir_):\n",
    "    print(p)\n",
    "    if p.endswith('.pt'):\n",
    "        tensor = torch.load(os.path.join(dir_, p), map_location=\"cpu\", weights_only=True)\n",
    "        print(tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
